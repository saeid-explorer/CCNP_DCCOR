====================================================================================== CCNP DCCOR - CBT Nuggets - Jeff Kish ================================================================================


~~~~~~~~~~~~~~~~~ Explain BGP Concepts~~~~~~~~~~~~~~~~

** BGP Concepts
  - BGP is an EGP(Exterior Gateway Protocol)
  - EGP is designed to be used between larger networks versus IGP(Interior Gateway Protocl) which is designed to be used within a network(e.g. OSPF)
  - IGPs are mode for speed(They converge rapidly) compared to EGPs which focus on Scale
  - IGPs are Distance Vector or Link State protocols but BGP is Path Vector protocol 
  - BGP has 10+ path attributes which are used for routing

** BGP Peerings
  - AS(Autonomous System) is 16bit number which accounts for 65K AS
  - There are two types of Peerings:
    - eBGP: peering between routers in different ASs
      - AD(Administrative Distance) is 20
    - iBGP: peering which happens within an AS
      - AD(Administrative Distance) is 200
  - BGP peerings are formed across a TCP connection on port 179
    - This means two routers without a direct connection can still form relationship
  - Peers are identified by a Router ID(32bit)
    - Router ID can be assigned manually or Automatically(using the highest Loopback IP or highest other Interface IP address)


** BGP Path Selection
  - Note: Path must be Reachable
  - Here are path selection criteria/attributes sorted by their priority:
      1) Weight
          - the higher the better
          - Cisco-only parameter
          - specifically local to the router
      2) Local Preference
          - the higher the better
          - it's globally shared within an AS
          - in other words, it's a weight that is shared
      3) Locally Originated
          -  prefer the local path to the neighbors' advertised path
      4) AS-PATH
          - the less/shorter the better
          - for example AS-path 400 wins over AS-Path 200-300
      5) Origin
          - the lower the better
          - Origin explains how a Route got into BGP in the first place
            - for example we might use the "Network" command which injects the route natively into BGP (marked by "i" in Routing Table)
            - we might also see "?"(i.e. Incomplete) in Routing table which means the Route is redistributed into BGP 
            - so "i" Origin is prefered over "?" Origin
      6) MED(Multi-Exit Discriminator)
          - the lower the better
          - it's actual cost or metric that we get in BGP
          - this is used to tell others how to come back into my network
          - for example when I have a Router on the Edge of my AS and I have 2 different Paths 2 different Routers in a different AS,
            I can use MED to influence the inboud direction of the packet
      7) eBGP > iBGP Peer
          - Routes received from eBGP peers are preferred over iBGP Routes
      8) Next-Hop - IGP Metric 
      9) Last chosen
      10) Peer Router-ID (Optional)
          - the lower the better
      11) Cluster Length
          - the lower the better
          - Relates to RR(Route Reflector)
      12) Peer IP Address
          - the lower the better


** MP-BGP (MultiProtocol BGP)
  - BGP can route IPv4 and IPv6
  - BGP is an Information Carrying protocol which can protocol lots of information types(e.g. IPv4/IPv6 Multicast, Mac Address)
  - we set the protocol that we want to carry as a subcommand under "router bgp <As#>" command
      - e.g.:
        - router bgp 65500
          - address-family ipv6


~~~~~~~~~~~~~~~~~ Explain Multicast Routing ~~~~~~~~~~~~~~~~

** Multicast Traffic Concepts
  - we have three components
      - Multicast source
      - Multicast receiver
      - Group Address: A class D IP Address that we are sending this multicast traffic stream to
        - Class D Address is in the range of: 224.0.0.0 to 239.255.255.255
        - 239.0.0.0 is the range of private multicast group IDs
  - different types of multicast traffic groups:
        1) Any-Source Multicast(ASM)
            - It's the situation in which we don't care which Source gives us the traffic stream
        2) Source-Specific Multicast(SSM)
            - Allows us to reach out to very specific source on the network and request the source that way
            - It greatly simplifies our multicast deployment because the host knows where exactly the source is
        3) BiDirectional
            - Designed for the situations when we have a lot of sources and a lot of recipients

  - L2 and L3 Multicast
      - L3 Multicast is going to be managed by a protocol called PIM(Protocol Independent Multicast)
          - The goal of PIM is to create Loop-free topology through the entire network that we call it Tree
          - This tree is going to connect the sources to the destinations in a loop-free way inside the network
      - L2 Multicast uses IGMP(Internet Group Management Protocol)
          - IGMP is a protocol that allows clients to request connections to specific multicast streams
          - We can also use a concept called IGMP Snooping in order to prevent Broadcast behaviour



** Protocol Independent Multicast(PIM)
  - PIM's responsibility is to connect the Sources with whatever clients want to receive that particular Multicast stream
  - It's protocol independent because we're relying on a Unicast Routing Protocol to already have been established for us to have
    converged routing(e.g. OSPF, EIGRP) in the Unicast Environment
  - PIM's domain of operation is from Ingress router to Egress router
  - Its responsible for L3 domain of Multicast
  - PIM routers should start by forming PIM neighborship with one another
  - Routers are going to discover each other with a multicast IPv4 address of 224.0.0.13 on IPv6 of FF02::D
  - Multicast Routes are created when a new Source shows up and sends Multicast or when a Receiver shows up and want to join a particular Multicast group
  - command to show Multicast Routing Table:
      - show ip mroute
  - 2 different types of Multicast routes on Multicast Routing Table
      - "S" stands for Source, "G" stands for Group, "*" means we don't know what the source is
      1) (S,G)
          - They are going to be established before (*,G) group in many cases as going to tell us what to do when we finally see where the source lives
          - e.g.: (10.1.1.1, 239.1.1.1)
      2) (*,G)
          - These Routes are kind of like Template that tell us what to do when the (S,G) Routes finally arrived
          - If the client first sends a Join request(to join a Multicast group) to the router
          - Once the router received the request, it's gonna build (*,G) entry into the Multicast Routing Table(e.g. (*,239.1.1.1) )
          - Note: with any Multicast Route, we need to identify Incoming Interface(II) and the Outgoing Interface List(OIL)
          - for (*,G) Routes, II is always "NULL" because in this case we don't have a Source

  - 2 Operation Modes of PIM
      1) DENSE Mode
          - has a Flood and Prune methodology(Flood the traffic and Prune it back)
          - We don't use DENSE Mode onto Cisco Nexus operations
      2) SPARSE Mode
          - On the Cisco Data Center we have only Sparse Mode available to us
          - It's going to use a lot of Intelligence to figure out where those Sources are
    - These Modes are going to be how we discover where the Sources are 


** PIM SPARSE Mode Operation
  - Rendevous Point(RP): This is going to be the contact for both the Sender and the Receivers
  - We are going to call a Router as RP and it will be known by every single router in the network
  - So every Router(Sender&Receiver) inform RP about their Needs/Requests
  - If the receiver shows up first:
      - Receiver will send a join request to its upstream Router and that router sends the join request to RP to join a specific Multicast group(e.g. 239.1.1.1)
      - RP gonna install (*,G) entry in its Multicast Routing table
  - If the Source show up:
      - It will ask RP for sending a Multicast traffic for specific Multicast group
      - The RP will install (S,G) Route in its Multicast Routing table
      - Then the RP will bind (*,G) Routes with (S,G) Routes and send the Multicast Stream to the given Clients in the specific Group


** PIM Roles
    1) Rendevouz Point(RP)
        - Contact for connecting the Sources and Receivers
        - We can Statically set any Router as RP or we can Automatically set a Router as RP in one of two following ways:
            1) Auto-RP
                - deployed by Cisco
                - based on SPARSE-DENSE Approach of PIM which is now obsolete in the Data Centers 
            2) BSR(BootStrap Router)
                - it's included with PIMv2
                - It's industry standard that's built-into PIM now
                - It uses PIM messaging sent to 224.0.0.13
                - BSR is a Mapping Agent(MA). It accepts all the messages from the Routers that want to be RP and sends those messages to 
                  the rest of the network via native PIM messaging
            3) Anycast RP
                - It leverages the concept of Anycast
                - It's going to use exact same IP Address on two different Routers
                - Here we have redundancy
                - The two routers(with same IP) use a protocol called MSDP(Multicast Source Discovery Protocol) to communicate with each other
                  to allow each other to know about any Sources that they become aware of. This way we have one Synchronized database of Sources
                  between two RPs
    2) PIM DR(Designated Router)
      - This is when we have two Routers in one network segment
      - PIM DR is the Router that is responsible for sending client join Requests to the upstream Router
      - it's elected based on the highest IP Address on that subnet
    3) PIM DF(Designated Forwarded)
      - if we receive a Multicast Stream on two different Routers that are on the same Segment which should forward the Stream to Downstream device(Router/Client)
      - PIM DF is responsible for forwaring this Multicast Stream
      - In this case the two competing routers are going to enter into a process called PIM ASSERT
      - The Router which wins the PIM ASSERT process gonna forward the Multicast Stream




** PIM Configuration
    - We have two types of configuration to do:
      1) Global Configuration
          # Feature pim
          # ip pim rp-address
      2) Interface-level Configuration
          # ip pim sparse-mode
          - To enable Authentication
            # ip pim hello-authentication ah-md5 <password>
          - To make a specific Router to be DR
            # ip pim dr-priority
          - To configure Auto-RP and select a Router as RP candidate
            # ip pim auto-rp rp-candidate
          - To configure BSR and select a Router as RP candidate
            # ip pim rp-candidate
          - To configure a Router to be as MA(Mapping Agent) in Auto-RP
            # ip pim auto-rp mapping-agent
          - To configure a Router to be as BSR in BSR mode
            # ip pim bsr-candidate
          - To enable the forwarding of RP related traffic on Auto-RP mode
            # ip pim auto-rp listen forward
          - To enable the forwarding of RP related traffic on BSR mode
            # ip pim bsr listen forward

    - show Multicast Routing Table
          - show ip mroute

    - show pim info on interface level
          - show ip pim neighbor
          - show ip pim interface

    - flush all Multicast Routes out and restart all pim adjacencies
          


** IGMP and MLD
    - The role of IGMP is to allow the hosts to request for participating a particular Multicast Group
    - The role of MLD is the same as IGMP but for IPv6
    - host/client gonna send a request for joining the Multicast Group. This request is called "IGMP Report" and its destined to the Multicast Group Address(e.g. 239.1.1.1)
    - IGMP Querier
      - if we have more than one Router, the router with lower IP Address will be IGMP Querier 
    - IGMP has two versions that are supported on Cisco Nexus Switch
      - IGMP v2
      - IGMP v3
        - supports SSM(Source-Specific Multicast). i.e. when the host sends IGMP Report, it says from which Source it wants to receive Multicast Stream
    
    - MLD has two versions that are supported on Cisco Nexus Switch
      - MLD v1
      - MLD v2
        - supports SSM

    - IGMP Snooping
      - Multicast in layer 2 acts like broadcast, so we use IGMP Snooping to make it act like Multicast
      - using IGMP Snooping, the switch sends the Multicast Streams only to the hosts that have requested


** IGMP and MLD Configuration
    # feature pim/pim6
    - under the interface
        # ip pim sparse-mode
        # ip igmp version 3
        # ip igmp join-group
          or
        # ipv6 mld join-group
    - Validating commands
        # show ip igmp interface
        # show ipv6 mld interface
        # show ip igmp snooping groups
        # show ipv6 mld snooping statistics int vlan <vlan#>



~~~~~~~~~~~~~~~~~ Explain Virtual Port Channel ~~~~~~~~~~~~~~~~
  
  - Nexus Port-channels are different from IOS devices
  - Configuration options are static(ON) and LACP(active/passive)
  - Load-balancing across port-channel links is hash-based

  * Note: In VPC we can only support exactly 2 upstream Nexus Switch
  * Note: VPC is only L2

  * VPC Domain
    - it is assigned a unique number inside your datacenter environment

  * Peers
    - Two Nexus devices in the VPC domain are called PEERS
    - one of the devices is Primary Peer and the other is Secondary Peer

  * KeepAlive Link
    - it's going to be a dedicated connection between peers that is part of its own VRF
    - This should be a L3 connection
    - Peers will send KeepAlive messages to one another to make sure whether the other side is up

  * Peer-Link
    - it's a connection between peers which is usually several physical connections with Standard Port-Channel
    - peers will use this link for control plane information
    - Control Plane uses a Protocol called CFS(Cisco Fabric Services)
    - any VLAN that we extend along a VPC must be included on the Peer-Link
    - The general practice is not to restrict VLANs on the Peer-Link

  * VPC Member Ports
    - The ports that are part of VPC

  * VPC Orphan Ports
    - Any port in a VPC environment that aren't part of VPC

  * Note: VPCs give us the resiliency and storage connections needs in the DC

  * Consistency Checks of VPC
      - If there were inconsistencies in Type 1 checks, the VPC wouldn't come up
      - If there were inconsistencies in Type 2 checks, Port-Channel will come up but you'll receive some errors and problems
      1) Type 1
          - STP Type
          - Access/Trunk
          - Native VLAN
          - Speed

      2) Type 2
          - SVI UP/DOWN
          - QoS Configuration
          - VLAN Allowed List  


    * Note: VPC traffic is hashed, but only local VPC port members are considered

    * Note: Loop Prevention blocks Peer Link packets from forwarding out a VPC

    * Note: FHRPs run in an Active/Active state to resolve black hole scenarios

** VPC and STP
    - In normal operation, the VPC Primary handles BPDUs on VPCs
    - Peer-Switch is a concept which means two VPC switch peer in order to be considered one logical switch
    - Using Peer-Swtich, the peers share a bridge ID for VPC links and that enables both of them to send and receive BPDUs
    - Peer-Switch is designed for when one of the peers is STP Root

** VPC Topologies
    - Bow-Tie
      - it's a topology in which two downstream switch connect to two upstream routers using VPC
      - we have two paris of switches
      - The link between the pairs is L2

    * Note: L3 adjacencies to the VPC peers should not be formed over a VPC
    * Note: Use dedicated L3 links when routing adjacencies to the peers are required

** Configuring a VPC Domain
    - do the following configuration on both Nexus Switches:
      - feature vpc
      - vpc domain <a unique number>
      - peer-keepalive destination <destination IP of keepalive link> source <Source IP of keepalive link>
    - to validate the config use the following command and check "vPC keep-alive status"
      - show vpc
     - Go inside the interface of peer-link on both Switches
      - int <interface_number>
          - switch mode trunk
          - channel-group <number> mode <on/active>
      - Go to the port-channel that you've created
        - int port-channel <number>
          - vpc peer-link
      - To validate the peer-link, see if "peer status" is "peer adjacency formed ok" using following command
        - show vpc
      - Go to the interfaces that you want to configure as the members of vPC ports to downstream swtich
        - int <interface_name>
          - switchport mode trunk
          - channel-group <number> mode active
              * Note: you should enable LACP feature first(i.e "feature lacp")
      - Then go to the newly created port-channel
        - int port-channel <number>
          - vpc <number>
              *Note: for the human-readability it's better to match vpc and port-channel number of vPC member ports
      - Then go to downstream switch(e.g. Catalyst) and enter into ports that will join the vPC
        - int range <interface range>
          - switchport trunk encapsulation dot1q
          - switchport mode trunk
          - channel-group <number> mode passive

** VPC Roles and Failover
    - one of the Nexus VPC switches will be Primary and the other will be Secondary
    - Primary and Secondary roles will be determined via an Election
    - To see the role of each Nexus switch
      - show vpc role
    - Election will be done one time when vPC domain comes up
    - Election will be based on Priority and Mac Address
    - There is no Preemption. It means if the primary failed, it won't get Primary again after comming up

    * Failure Scenarios
      1) Peer-Link is down, but KeepAlive link is up
        - In this case, the secondary switch will shut its vpc link down

      2) Peer-Link and KeepAlive links are both DOWN
        - In this case, secondary switch will keep its VPC up and it will assume the role of operational primary, it
          means it's still configured as secondary, but it's acting as if it's primary

      3) Peer-link is UP, but KeepAlive links are Down
        - we have to solve KeepAlive link problem right away, but it's not going to affect Data Plane Operations

** Configuring VPC Roles
    - to make a nexus switch to be VPC Primary, use the priority command and set it to a lower priority, and set the secondary swith to have higher priority
    - e.g.:
      (in primary switch)
      - vpc domain <domain_number>
        - role priority 10
      (in secondary switch)
      - vpc domain <domain_number>
        - role priority 20
    * Note: we should either shut down vpc links and "no shut" them again or do the following to take effect:
        - vpc role preempt


** VPC Configuration Options

  * ARP Syncronization
    - This gonna make sure that we're dynamically and actively/proactively exchanging ARP entries of the Nexus Switches with one another
    - That way in a failure event, we don't have to re-ARP for bunch of entries that we weren't even using before the failure
    - We should enable ARP Sync in VPC domain
    - The command to enable it
      - vpc domain <number>
        - ip arp synchronize

  * Auto Recovery
    - It's one of those configuration parameters that is actually enabled by default
    - In Auto Recovery we gonna wait for 240 seconds(i.e. 4min) in hope that the other failed switch(primary) comming online
        - if the other failed switch didn't came up in 240 seconds, the switch will take the Primary role

  * Peer Gateway
    - It extends FHRP black hole prevention to SVIs
    - The command to enable it
      -  vpc domain <number>
        - peer-gateway



  ~~~~~~~~~~~~~~~~~ Describe Data Center FHRPs ~~~~~~~~~~~~~~~~

** First Hop Redundancy Protocols(FHRPs)
    - these protocols allow for network-based gateway redundancy
    - using these protocols we can have two gateways for our clients and assign a single VIP for the gateways
    - Protocols:
      - HSRP(Hot Standby Router Protocol) 
        - Cisco proprietory
        - two versions: v1(original version), v2(optimized version)
        - default version of HSRP on Cisco NX-OS is v1

      - VRRP(Virtual Router Redundancy Protocol)
        - It's industry/open standard
        - versions: v1(obsolete), v2 and v3
        - default version of VRRP on NX-OS is v2

      - GLBP(Gateway Load Balancing Protocol)
        - Cisco proprietory
        - it's focus is on load balancing
        - it's supported on Nexus 7K but not on Nexus 9K



** Hot Standby Router Protocol(HSRP)

  - In HSRP we will have groups containing routers
  - groups will be configured on Interface
  - Each group will be assigned an ID
  - Each router will be placed in one of these states:
    - Active
    - Standby
    - Listen

  * Priority
    - it's a configurable value in the range of 0-255
    - Default is 100
    - If the priority of the routers is the same, the router with "Highest IP Address" wins(becomes Active Router)

  * Timers
    - The routers send messages to one another
    - sending these messages is according to "Hello Timer"
    - Hello Timer is by default 3 seconds
    - Hold Timer is by default 10 seconds
    - Hold Timers says us how long we can go without receiving a hello message before we assume the other side is down
    * Note: we can use BFD(Bidirectional Failure Detection) algorithm to detect the failures even sooner (down to 50ms)

  * Preemption
    - it needs to be enabled if I want a router to dynamically take over(become Active router) for example as a result of changing priority value
    - It's disabled in HSRP by default

  * Security
    - Authentication
      - we can for authentication between routers inside a HSRP group
      - The password can be in two froms
        - Plain text
        - MD5

  * HSRP v2
    - It has the possibility of more group IDs(4096)
    - We are able to tune our timers down to Millisecond range(as low as 250ms)
    - Multicast address is going to change
      - In HSRP v1, Multicast Address is 224.0.0.2
      - In HSRP v2, Multicast Address is 224.0.0.102 

  * Note: HSRP is Active/Active in VPC topologies


** Virtual Router Redundancy Protocol(VRRP)
    - Unlike HSRP, the roles are:
      - Master
      - Backup

    - Priority
      - in the range of 1-254

    - VIP
      - we can select the IP address of Master as VIP

    - Preemption
      - is enabled by default

    - Group ID
      - In both VRRP v2 and v3 there are 256 Group IDs


** FHRP Object Tracking
    - Object Tracking enables FHRPs to react to indirect failures
    - Devices can track a number of different parameters
      - Route
      - Line Protocol
      - SLA
      - List

    - While tracking the parameters
      - when a problem is detected, the device will decrement its priority
      - in this case we expect the backup/standby router to take over the Master/Active Role
      - preemption should be enabled


** Configuring HSRP
    - first enable the feature on Nexus Switch
      - feature hsrp
    - then configure hsrp on the interface as the following example
      - int vlan 1
        - hsrp version 2
        - hsrp 1
          - ip 192.168.1.254 (VIP)
    - to change priority and turning Preemption on
      - int vlan 1
        - hsrp 1
          - priority 110
          - preempt

    - to change Hello time and Hold time(e.g. 1s for Hello time and 3s for Hold time)
      - int vlan 1
        - hsrp 1
          - timers 1 3

    - to change Hello time and Hold time(e.g. 250msec for Hello time and 750ms for Hold time)
          
    - hsrp verification
      - show hsrp brief


** Configuring VRRP
    - first enable the feature on Nexus Switch
      - feature vrrp
        or
      - feature vrrp3

    - then configure vrrp on the interface as the following example
      - int vlan 3
        - vrrp 3
          - address 192.168.3.254 (VIP)
          - no shut
            - we have to do no shut, unless vless won't get enabled

     - vrrp verification
      - show vrrp

    - to change priority and turning Preemption on
      - int vlan 3
        - vrrp 3
          - priority 110
          - preempt

    * Note: When configuring VRRP to use a rouer's physical IP address (as VIP), the priority value of 255 is automatically assigned



~~~~~~~~~~~~~~~~~ Explain Overlay Transport Virtualization(OTV) ~~~~~~~~~~~~~~~~

  ** OTV Architecture
    - OTV is designed to extend L2 to many different locations across L3 boundaries (Data Plane Operation)
    - It relies on IS-IS routing protocol to do what is called as MAC Routing (Control Plane Operation)

    * Join Interface
      - It's physical or logical interface that connects us to our L3 Network(It's where IP Address lives)

    * Internal Interfaces
      - They are usually L2 extensions(usually Trunk connections) to the rest of the Data Center(Where hosts and all the entities on Internal network connect to)

    * Overlay Interface
      - It's a logical Tunneling interface that's going to exist on the Edge device
      - This is how we are going to form our tunnels across the L2 boundary
      - we are going to form a tunnel between the overlay interfaces of two edge devices on different Data Centers
      - They don't have IP addresses set on them, instead they borrow the IP of Join Interfaces for source and destination of OTV Tunnel
      - The tunnel is going to carry L2 Frame inside L3 Packet

    - The Cisco Platforms which support OTV
      - Nexus 7K
        - The Line Cards must be either M series or F3 or above 
        - We need to deploy a seperate VDC in order to accomplish this
        - OTV itself requires a seperate License

      - ASR 1000
      - CSR1000V
      - ISR 4K 



** OTV Control Plane
  - IS-IS routing protocol is used in OTV Control Plane
  - IS-IS is often used on Service Provider space
  - IS-IS can carry any type of information
  - IS-IS is based on TLV(Type,Length,Value) fields(i.e. we don't have to carry Routes, we can carry MAC Addresses)
  - Cisco is going to do the IS-IS config for us on the backend(automated by OTV)
    - we can also manually set Timers or Authentication
  - OTV shares updates as MAC addresses are learned and pulled
  - each devices will advertise the MAC addresses in its CAM table to the other devices
  - Adjacencies are formed via the control-group Multicast address ideally
  - For unicast transports, we use adjacency servers to form adjacencies



** OTV Data Plane
  - Unicast traffic is handled via unicast in the transport network
  - BUM(Broadcast, Unknown Unicast, Multicast) traffic is also sent via unicast to all destinations if transport is unicast
  - With a multicast transport, BU traffic is sent via control-group ASM(Any Source Multicast) address
  - Multicast(L2) traffic uses SSM(Source-Specific Multicast), which requires IGMPv3 on the Join Interface



** Multihoming and Authoritative Edge Devices(AEDs)
  - BPDUs are not propagated across the OTV overlay
  - One of the devices in each DataCenter/Subnet will be assigned as AED for a particular VLAN 
  - AED is going to affect both Control Plane and Data Plane
  - In Control Plane perspective, A device will only advertise the MAC addresses for a VLAN if it's the AED for that VLAN
    - Also The device will only send the traffic to it's VLAN, if only it's the AED for that VLAN
  - VLANs are automatically mapped based on evens and odds
  - Backdoor links will require manual configuration
  - Site-VLAN
    - it has to be a dedicated VLAN that connects two AEDs as directly as possible
  - The AEDs can also become backup AED for other Vlans 


** OTV Design Considerations
    - ARP Optimization
      - reduces flooding behavior of ARP between sites
    - OTV adds 42Bytes to MTU(MTU becomes 1542Bytes)
    - OTV sets DNF(Do Not Fragment) bit in a packet
    - FHRP Isolation
      - We have to create VACL(Vlan Access Control List) and block FHRP Mac Addresses between FHRPs on two different Data Centers
      - FHRP Nodes both become Active and we gonna have AnyCast Gateway
  
  ~~~~~~~~~~~~~~~~~ Explain VXLAN and EVPN ~~~~~~~~~~~~~~~~

** VXLAN and VNI
  - There are some problems in traditional Data Centers
    - Vlans are limited (4K)
    - We have to use STP, FHRP, VPC which have their own problems
    - We have to use L3 PODs (L3 at edge) which are problematic

 - VXLAN helps us to better scale our network by two primary ways
    1) It will increase the number of Broadcast Domains that are available to us
    2) It's gonna extend L2 over L3

 - When we enable VXLAN on a switch, it's become known as VTEP(VXLAN Tunneling EndPoint)
 - One VLAN is going to be mapped to a single VNI(Virtual Network Identifier)
 - Then each VNI is going to be extended through a L3 boundary and connect to another VTEP which receives the 
   VNIs which has encapsulated the traffic that VXLAN enabled us to send.
 - Then the destination VTEP is going to translate VNI back into the individual VLANs that were originally
   connecting with.

 - VXLAN header is a 8 Byte header
    - 1B -> Flags
    - 3B -> VNI (almost 16Million VLANs)
    - 4B -> Reserved


  - Scoping the VNIs
    - There are two primary ways of scoping VNIs
      1) Network Scope
      2) Local Scope(locally significant VNI)
      
      - Network-wide scope means that if we were to apply a VNI to a particular VLAN, the question we're
        going to ask is whether this VNI is significant only to the local switch or to the entire Network.
      - Scoping Network-wide means that the specific VNI will also get mapped to the same Vlan on the
        destination VTEP
      - Benefit of Network-wide Scoping is that it's SIMPLE(i.e. VNI gonna point to the same VLAN in the
        entire Network)
      - Downside of Network-wide Scoping is Management. Because we have to be worry about that the specific
        VNI is reserved everywhere and we can't repurpose that VNI by accident anywhere in the Network.


** VXLAN and Leaf-Spine
  - Leaf-Spine is only doable because we have VXLAN

  - Leaf-Spine Architecture
    - We have two layers: 1st layer is Spine and 2nd layer is Leaf
    - we connect every leaf switch to every upstream spine switch
      - we don't connect leaf-to-leaf or spine-to-spine(unline Aggregation-Access Architecture)
    - All of the Intelligence lives at the LEAF layer, not the Spine layer
    - The Spine switch has only one task: to bring traffic in and send back out which enables us to switch traffic
      between Leaf swtiches as fast as possible
    - Leaf-Spine is designed for scaling to very large L2 Fabric in the DataCenters today

  - Leaf-Spine Scaling
    1) we have Independent Scaling of different resources in the data center
        - we have a Backbone layer(Spine Layer) and if we want to increase our Bandwidth or PPS(Packets Per Second),
          we add new Spine Layer Switch
        - If we need Ports(additional connectivity at the Edge), we simply add Leaf switches
    2) Declaring Two-Hop behaviour for all Traffic in Data Center
        - When a packet arrives on a Leaf Switch, it's guaranteed that regardless of where it's heading in the Data Center,
          it's going to get there in precisely 2 Hops(first to Spine and then to the destination Leaf Swtich)

    3) Services
        - We can make our Network Services available very seamlessly throughout the Infrastructure
        - We bring our services(Network Load Balancers, Cloud Gateways, NGFW, AnyCast Gateways) into a pair of leaf switches


  - Application of VXLAN in Leaf-Spine Architecture
        - The ideal situation in Leaf-Spine Architecture is that the connections between Leaf to Spine to be L3(to be able
          to scale this architecture out) to be able to scale out. Otherwise in L2 connections STP gonna cause lot of problems
        - In this case, we can have two hosts on two different Leaf switches which are in the same VLAN. Then we can have these
          subnets stretched out(L2) because they are in the same VLAN and yet our Fabric is L3



** VXLAN Data Plane
    - VXLAN gonnna extend L2 over L3 using Encapsulation
    - The L2 Frame gonna come to source VTEP. Then it will be encapsulated into VXLAN Header
    - Then it will be encapsulated inside UDP Header which has Destination Port of 4789
    - Then It will be encapsulated into IP Header which has Source and Destination IP of Source and Destination VTEPs
    - Then it will send the Encapsulated packet over L3 to the destination VTEP
    - Destination VTEP gonna decapsulate it and get the packet
    - Fragmentaion is not supported by VXLAN
    - Remember increasing MTU

   - MTU increase required on the VXLAN underlay network
      - IP header       -> 20B
      - UDP header      -> 8B
      - VXLAN header    -> 8B
      - Ethernet header -> 14
      - So totally we should add 50 Bytes to the size of Data packet to get our MTU size

** VXLAN Control Plane - Flood and Learn
    - VXLAN lacks a native, dedicated Control Plane, instead it relies on Flood and Learn
    - Traffic is flooded via Multicast across the backbone to all VTEPs
    - Once a destination is learned, the ingress VTEP can forward as unicast
    * BUM(Broadcast, Unknown Unicast, Multicast) Traffic will be sent using  Multicast tree
    - IR(Ingress Replication) is where we have Unicast Backplane
      - it's an alternative method of control plane(for Multicast) which is rarely used
      - we have to manually map VNI to VTEP which takes lots of work
      - we gonna replicate lots of traffic


** VXLAN Control Plane - Ethernet VPN (EVPN)
    - BGP is more of a Information Carrying Protocol(rather than simply a Routing Protocol)
    - MP-BGP(Multi-Protocol BGP) can carry IPv6, MPLS or even MAC and VNI
    - EVPN deploys MP-BGP to exchange IP/MAC/VNI info among VTEPs
    - BGP should also include a RR(Route Reflector) as part of EVPN designs
    - EVPN brings enhancements like Peering(Authentication) and ARP Suppression(Prevent flooding ARP Requests)
    - BUM traffic continues to use Multicast for multi-VTEP delivery


~~~~~~~~~~~~~~~~~ Configure Overlay Transport Protocol(OTV) ~~~~~~~~~~~~~~~~

** Setting up the OTV Environment
- Configurations on the device on the transit environment of OTV(e.g. CSR1000v)
  - configure multicast routing on the device on the transit environment of OTV(e.g. CSR1000v)
      # ip muticast-routing distributed
  
    - Assign an IP address to it's multicast port
      # ip pim rp-address 1.1.1.1
  
    - enable pim sparse-mode on loopback interface
      # ip pim sparse-mode
      # ip address 1.1.1.1 255.255.255.255

    - enable igmp version 3 on the interfaces towards two other downstream devices(e.g. Nexus 7K)
      # ip igmp version 3


- Configuring Internal and Join Interfaces of OTV on Cisco Nexus 7K
  - Go to the L3 interface facing the upstream CSR1000v and configure it as JOIN interface
    # int e2/1
      # ip add 10.0.0.2/30
      # mtu 9216
      # ip igmp version 3
      # no shut 

  - Go to the interface facing the Local Network and configure it as Internal interface
    # int e2/2
      # switchport mode trunk
      # switchport trunk allowed vlan 10-20
      # no shut
      # exit

  - Create vlans
    # vlan 10-20
    # vlan 5
    # exit

      
** Configuring OTV
  - configuration on Nuxus 7K
    - Global Configuration
      # feature otv
      # otv site-vlan 5
        # exit
      # otv site-identifier Ox1
        - site identifier is always the same for all of the devices in a particular location

    - Interface-level Configuration(for Multicast)
      # int overlay 0
        # otv join-interface eth2/1
        # otv control-group 239.1.1.1
        # otv data-group 232.1.1.0/28
        # otv extend-vlan 10-20
        # no shut

    - Interface-level Configuration(for Unicast)
      # int overlay 0
        # otv join-interface eth2/1
        # otv adjacency-server unicast-only
        # otv use-adjacency-server 10.0.0.2
        # otv extend-vlan 10-20
        # no shut

    - verification
      # show otv adjacency
      # show otv route



** Configuring OTV FHRP Blocking
    - blocking FHRP using VACL(VLAN Access Control List)
      # ip access-list FHRP_IP 
          # permit udp any 224.0.0.2/32 eq 1985  -> HSRP v1
          # permit udp any 224.0.0.102/32 eq 1985 -> HSRP v2
          # permit 112 any 224.0.0.18/32
      # ip access-list IP_ALL

    # vlan access-map BLOCK_FHRP_VACL 10
       # match ip address FHRP_IP
       # action drop
       # exit

    # vlan access_map BLOCK_FHRP_VACL 20
      # match ip address IP_ALL
      # action forward
      # exit

    # vlan filter BLOCK_FHRP_VACL vlan-list 10-20
        
  - Configure Mac Advertisement
    # mac-list OTV_FHRP_MAC seq 10 deny 0000.0c07.ac00 ffff.ffff.ff00
    # mac-list OTV_FHRP_MAC seq 20 deny 0000.0c9f.f000 ffff.ffff.f000
    # mac-list OTV_FHRP_MAC seq 30 deny 0000.5e00.0100 ffff.ffff.ff00
    # mac-list OTV_FHRP_MAC seq 40 permit 0.0.0 0.0.0

    # route-map OTV_FHRP_MAC_RM permit 10
      # match mac-list OTV_FHRP_MAC_RM
      # exit

    # otv-isis default 

    # vpn overlay0
      # redistribute filter route-map OTV_FHRP_MAC_RM
      # exit

~~~~~~~~~~~~~~~~~ Configure VXLAN ~~~~~~~~~~~~~~~~

** Setting up the VXLAN Environment
  - Configuring CSR1000v(as Spine Switch)
      - enabling multicast routing and setting pim RP(Rendevouz Point) IP
          # ip multicast-routing distributed
          # ip pim rp-address 10.1.1.100
      
      - Set pim-mode to sparse on the interfaces
          # ip pim sparse-mode

      - increase MTU(considering Jumbo frames and VXLAN header)
          # int eth1/1
            # mtu 9216

  - Configuring Nexus 9K (as Leaf Switch)
      - increase MTU(considering Jumbo frames and VXLAN header)
          # int eth1/1
            # mtu 9216

      - enable Multicast
          # feature pim

      - set RP(Rendevouz Point) to be upstream CSR1000v Router
          # ip pim rp-add 10.1.1.100

      - Set pim-mode to sparse on the interfaces
          # ip pim sparse-mode

      - configure vlans and place downstream ports on vlans
          # vlan 10,20

          # int eth1/1
            # switchport access vlan 10
            # switchport mode access
            # no shut

          # int eth1/2
            # switchport access vlan 20
            # switchport mode access
            # no shut


** Configuring VXLAN
  - Configuration Cisco Nexus 9K as Leaf Swtiches
    * Note: we should convert our Nexus 9Ks to VTEP(Virtual Tunnelling Endpoint)
      - To do that, we should configure NVE(Network Virtualization Edge) Interface
      - Then we map the VNI to either a physical or virtual interface(e.g. Loopback)
      - This for example loopback interface, is the interface that we use to form our VXLAN relationship/Tunnel
      - So, because NVE doesn't have an IP address itself, it will use loopback's IP address as source/destination of Tunnel

    - Enable VXLAN configuration
      # feature nv overlay
      # feature vn-segment-vlan-based

    - mapping VLANs to VNIs
      # vlan 10
        # vn-segment 100000
        # exit

      # vlan 20
        # vn-segment 200000
        # exit

    - verification command for vlan-VNI mapping
        # show vxlan

    - create NVE interface and set an interface as a member of VNI and set multicast group IP
        # int nve1
          # source-interface loopback 0 (loopback interface should have already configured with an IP Address)
          # member vni 100000
            # mcast-group 239.1.1.1
          # member vni 200000
            # mcast-group 239.1.1.2
          # no shut

    - verification of NVE peers
        # show nve peers

    - verification of NVE/VNI association
        # show nve vni



~~~~~~~~~~~~~~~~~ Configure Ethernet VPN (EVPN) ~~~~~~~~~~~~~~~~

** Configuring EVPN
  - EVPN is a control plane protocol for VXLAN which uses BGP
  
  - Configuring CSR1000v as upstream Spine switch
    # router bgp 65000
      # neighbor 10.0.0.2 remote-as 65000 (neighbors are two downstream nexus switches as leaf switches)
      # neighbor 10.0.0.6 remote-as 65000
      # address-family ipv4
        # neighbor 10.0.0.2 activate
        # neighbor 10.0.0.6 activate
        # neighbor 10.0.0.2 route-reflector-client (this is to configure CSR1000v as an RR for two Neighbors(Nexus Swtich))
        # neighbor 10.0.0.6 route-reflector-client

  - Configure two Nexus 9k Switches as Leaf
    # int loopback 1
      # ip address 1.1.1.1/32
      # no shut
      # exit

    # feature bgp

    # router bgp 65000
      # address-family ipv4 unicast
        # network 1.1.1.1/32
        # exit
      # neighbor 10.0.0.1
        # remote-as 65000 
        # address-family ipv4 unicast
        # exit

  - Configuring CSR1000v as upstream Spine switch
    # router bgp 65000
      # address-family l2vpn evpn
        * Note: l2vpn address-family carries all of the EVPN information(IP/MAC)
        # neighbor 10.0.0.2 activate
        # neighbor 10.0.0.6 activate
        # neighbor 10.0.0.2 route-reflector-client
        # neighbor 10.0.0.6 route-reflector-client
        * Note: Community Attributes are the Attributes which allow tag a route with any kind of value that effectively tells another router what to do
                witch that route
        * Note: EVPN relies on Extended Community Attributes
        # neighbor 10.0.0.2 send-community extended
        # neighbor 10.0.0.6 send-community extended


  - Configure two Nexus 9k Switches as Leaf
    * Note: In order to have access to that L2VPN BGP Address-family, we actually need to enable EVPN control plane option for us
      # nv overlay evpn

    * Note: RD(Route Distinguisher) make a Route unique within the BGP table
    * Note: RT(Route Target) tells us where to send the unique Routes
            - Targets are determined by a TAG. We are going to export the route with a particular Tag so that we can Import that particular Route
              with a Tag on the other side
            - Whitin the world of EVPN, we can use the concept of Auto. Auto works within IBGP and Cisco derives as their own RD and RT based on internal values
    # vni 100000 L2
      # rd auto
      # route-target import auto
      # route-target export auto
      # exit

    # vni 200000 L2
      # rd auto
      # route-target import auto
      # route-target export auto
      # exit

    # int nve1
      # host-reachability protocol bgp
      # no shut
      # exit

    # router bgp 65000
      # address-family l2vpn evpn
      # neighbor 10.0.0.1
        # address-family l2vpn evpn
          # send-community extended
          # exit

  - Configuring CSR1000v as upstream Spine switch
      - disable multicast
        # int gig1
          # no ip pim sparse-mode
        # int gig2
          # no ip pim sparse-mode

  - verification command for EVPN
      # show l2route evpn mac all
      
      
~~~~~~~~~~~~~~~~~ Apply Overlay Protocol ~~~~~~~~~~~~~~~~

** Overlay Design Considerations
  - Traffic Optimization
    1) GW
      - ANYCAST (EVPN)
      - FHRP Filtering (OTV)
    2) ARP
      - Suppression

  - Control Plane
    - OTV -> IS-IS
    - VXLAN -> EVPN

  - Loop Prevention
    - doesn't happen automatically, so we have to be very cautious specially on VXLAN environment
    - some preventive mechanisms such as BPDU GUARD are needed

** VXLAN and VPC
  - VXLAN configuration must match identically between the peers
  - VPC optional configurations are now required
    - Peer switch
    - Peer gateway
    - Arp synchronization
  - Failures are handled similarly, but consider upstream L3 failure
    - we can deploy L3 connection between our VTEPs
  - we can test resiliency with "system mode maintenance" command

** VXLAN Multipod and Multisite
  - Pod in a data center environment is an "availability domain"
  - Multipod
    - Multipod is built by merging multiple pods together under one fabric
    - BGP and EVPN are used
    - problems:
      - Scaling
      - Single Control Plane
      - We should have a Full Routing Table for all the different VTEPs
      - BUM traffic
      - less visibility of traffic
  - Multisite
    - each data center is considered to be a Site with some Pods
    - Border Gateway 
      - we convert a VTEP(Leaf or Spine switch) into a Border Gateway
      - Border Gateways terminate VXLAN tunnels from both sides
      - traffic between the sites are transferred via Border Gateways
      - using ANYCAST we can have 2 Border Gateways on each site
    - The following problems(of Multipod) are solved in Multisite
      - Scaling
      - Single Control Plane
      - Visibility

** OTV vs VXLAN
  - OTV and VXLAN both pass L2 and L3, but they are designed differently
  - OTV was designed for DCI(Data Center Interconnect) and it still has advantages
  - VXLAN can be used between VXLAN-enabled data centers
  - OTV benefits
    - can operate on a UNICAST service provider network
    - we can connect the data centers even if we have VXLAN on one side and not on the other
    - comes with integraded control plane(IS-IS)
    - can be viewed as a Simple design
  - OTV Downsides
    - don't need OTV if we have VXLAN enabled on both sides
    - we have to worry about the concept of STITCHING(Stitching VNIs to VLANs)
    - require specialized hardware(e.g. Nexus 7K or ASR1K and not Nexus 9K) and some licensing
  - Using VXLAN alone
    - we have to consider full mesh of VXLAN tunnels among all of our different VTEPs
  - Using VXLAN alongside OTV
    - we have to fully consider how we exactly map out VNIs to VLANS(Stitching)
    - This requires a Nexus 7K M3 Blade
    - We would have to rely on OTV Unicast
    - We have Scaling limitation of 3 Data Centers



~~~~~~~~~~~~~~~~~ Describe ACI Concepts ~~~~~~~~~~~~~~~~

** What is Cisco ACI(Application Centric Infrastructure)?
  - It's Cisco SDN solution for Data Center
  - SDN is a software-driven methodology for managing the infrastructure
    - These are usually going to be managed by Controllers
    - APIs are used 
      - REST(http) format - user will use this API
      - NETCONF API - devices will use this API to talk to each other
    - Overlay
      - VXLAN or other tunneling mechanisms are used to deploy Overlay
      - Underlay is specifically the hardware
  - The Controller that is used on ACI environment is called APIC(Application Policy Infrastructure Controller)
    - APIC is going to allow user to configure ACI via GUI or CLI  
  - ACI is built on the Nexus 9K line of switches



** ACI Resiliency and Simplicity
  - ACI offers resiliency via Leaf-Spine and VXLAN
  - ACI offers simplicity via APIC as a single point of access



** ACI Automation, Security, and Flexibility
  - Automation
    - ACI offers automation via REST APIs
    - No Human Error
    - Inefficient to do manual work

  - Security
    - ACI offers north-south and east-west security

  - Flexibility
    - ACI offers flexibility by extending policy to both physical and virtual devices



~~~~~~~~~~~~~~~~~ Describe ACI Manageability ~~~~~~~~~~~~~~~~

** Northbound APIs
  - APIC is going to be a cluster of usually 3 Cisco UCS servers, however we can even cluster more
  - The reason of having 3 on a cluster:
    - we are going to shard the data that's contained by the APIC across all of the different nodes
    - That is going to provide the resiliency we want on a management layer of ACI
  - The APIC is exclusively configured via northbound REST APIs
  - The GUI and CLI both serve as front-ends for the APIs
  - Cisco Tools we can leverage to help us begin programming ACI via APIs
    1) DOC
    2) Visore
       - allow us to learn about the attributes, variables, and DNs(Distinguished Name) of a Managed Object
    3) API Inspector
  * Note: Northbound of the APIC will be user


** Southbound APIs
  - Southbound of APIC would be all of the different devices(Nexus 9Ks)
  - We are going to use APIs between APIC and N9Ks
  - OpenFlow was the first industry standard protocol for SDN
  -  These APIs are gonna built on a protocol called OpFlex created by Cisco
  * OpFlex
    - it's an Open Protocol
    - it's a Declarative model

** ACI Programmability and Object Models
  - Network Programmability makes us more efficient and effective
  - Everything(e.g. Interface, VLANs, EPGs(EndPoint Groups)) in ACI is an Object
  - ACI uses and object model to facilitate API configuration
  - MGMT Information Model (MIM) is an object-oriented database which includes  MGMT Information Tree(MIT) 
    - Every one of the nodes on the Tree is called a Managed Object(MO)
    - Every MO has a Distinguished Name(DN)
      - e.g.: topology/pod-1/node-105
    - Relative Name(RN) is essentially a DN but shortened down
  - we can use the DN or RN to be called on API call




~~~~~~~~~~~~~~~~~ Describe ACI Components ~~~~~~~~~~~~~~~~
** ACI Architecture
  * Note: all the Intelligence in Spine-Leaf architecture lives on Leaf layer
  - The Nexus 9Ks were developed specifically for ACI
  - Nexus 9Ks can run in NX-OS mode until ready for ACI
  - The APIC consists of a 3-node UCS cluster

** Nexus 9500 Line Cards
  - Cloud Scale
    - refers to cisco-created silicon, used in modern 9700 line cards
    - Cisco 9700 series
    - EX
    - FX
      - Telemetry
      - MACSEC(Hop by Hop)
      - CloudSEC(VTEP by VTEP)
        - Encryption on VXLAN Tunnel
    - GX
      - 400G Interfaces
  - R-Series
    - exists as a merchant silicon option, NX-OS mode only
    - Cisco  9600 R series
    - LCs
    - FMs
  - Classic
    - Classic Line Cards used a merchant+ strategy, included the 9400-9700s
    - Gen1 Line Cards
      - 9400/9600 series
      - cannot run on ACI mode
    - 9500
      - leaf
      - ACI enabled
    - 9700
      - spine
      - ACI enabled

** Nexus 9500 Chassis Switches
  - Cisco 9504
    - includes 4 line cards(determined by last digit of the model)
    - 2 Line Cards and 2 Supervisors
    - 48p, 10G SFP
  - Cisco 9508
    - 6 Line Cards, 2 Supervisors
  - Cisco 9516
    - 14 Line Cards, 2 Supervisors
  - Supervisor is responsible for Network Functionality(e.g. OSPF, STP)
  - System Functionality(e.g. Power Mgmt, Temperature sensor) is offloaded to the System Controllers
  - Supervisors
    - A+
      - 4core, 1.8GHz CPU
      - 16GB RAM
      - 64GB SSD
    - B+ 
      - 6core, 1.9GHz CPU
      - 24GB RAM
      - 256GB SSD
  - Fabric Modules(FM)
    - The 9500s use a Fablic Module Architecture, with no midplane
    - each Fabric Modules applies to certain amount of Bandwidth to each Line Card
    - Fabric Modules is mounted vertically on back of the Chassis and increases the Backbone Bandwidth available to each line card  
    - we can support up to 6 FMs in a chassis
    - we need Minimum of 3 FMs



** Nexus 9300 Fixed-Config Switches
  - fixed-configuration switch means not-a-chassis-switch
  - Nexus 9300s work in NX-OS or as ACI spine or leaf, depending on the model
  - Options/Models
    - EX
    - FX
    - FX2
    - FX3
    - GX 
    - GX2

** Application Policy Infrastructure Controller(APIC)
  - APIC is the ACI controller for SDN environment
  - APIC is typically deployed in the cluster of 3, 5 or 7 nodes
  - There is two sizes: Medium and Large, sized by edge port count
  - APICs have a component called VIC(Virtual Interface Controller) 
    - These VICs are on Cisco UCS servers and are 10/25G SFP interfaces
  * Note: In order to upgrade a Nexus 9K from NX-OS mode to ACI mode
    - Complete wipe/reload


~~~~~~~~~~~~~~~~~ Data Center Transcivers ~~~~~~~~~~~~~~~~

** SFP and QSFP Technology
  * SFP(Small Form factor Plugable)
    - Smaller than GBICs
    - 1G(fiber/copper)
    - RJ45 support
  * SFP+
    - 10G
    - Form Factor similar to SFP
    - Backward Compatability
  * QSFP+
    - 40G
    - Breakout(convert to 4 x 10G links)

   * Note: SFP+/QSFP+ are outdated transceiver options

  * SFP28
    - supports 1G, 10G, and 25G connectivity
    - form factor similar to SFP
    - It's actually 28G but 3G is consumed by Encoding
    - RS-FEC Feature
      - it's a feature on SFP28 interfaces, that if not present, we are limited to 3 Meters
  * QSFP28
    - supports 40G, 50G and 100G
    - it's backward compatible and supports QSFP+ Transcievers


** 10G and 25 Transceiver Options
  * 10G SFP+ Modules
    - SFP-10G-T-X module
      - connect RJ-45 interfaces at 10G
    
    - SFP-10G-SR module
      - SR stands for Short Range
      - It's typically on Multi Mode Fiber(MMF)
      - supports FCoE
    - SFP-10G-SR-S Module
      - It's a lower cost option because of its worse Bit Error Rate(BER)  
      - does not support FCoE
    - SFP-10G-SR-X Module
      - It's for extented operating temperature range
    
    - SFP-10G-LRM Module
      - Primary meant for outside of Data Center(campus Environment)
      - It's on Multi Mode Fiber(MMF) 
    - FET-10G Module
      - Only works between Cisco Nexus 9K and 2K
    - SFP-10G-LR-S Module
      - LR stands for Long Range Fiber
      - It's on Single Mode Fiber(SMF)
      - supports link length of 10 kilometers
      - does not support FCoE
    - SFP-10G-LR Module
      - It's on Single Mode Fiber(SMF)
      - supports link length of 10 kilometers
      - supports FCoE
    - SFP-10G-LR-X Module
      - It's for extented operating temperature range
    - SFP-10G-LR10-I
      - It's meant for Industrial Operating Temperature Range

    - SFP-10G-ER-S Module
      - supports a link length of up to 40 kilometers
      - It's on Single Mode Fiber(SMF)
      - does not support FCoE
    - SFP-10G-ER Module
      - supports link length of 40 kilometers
      - supports FCoE
    - SFP-10G-ER-I Module
      - It's meant for Industrial Operating Temperature Range

    - SFP-10G-ZR-S Module
      - It's cisco-specific
      - supports link length of up to about 80 kilometers
      - does not support FCoE 
    - SFP-10G-ZR Module
      - supports link length of up to about 80 kilometers
      - supports FCoE

    - SFP+ Twinax copper cable
      - also called Direct-Attach Cables(DAC)
      - suitable for very short distances
      - offer a cost-effective way to connect within racks  and across adjacent racks
      - in lengths of 1, 1.5, 2, 2.5, 3, 4, 5 (called passive), or  7 and 10 meters (called active)
      - EMI(ElectroMagnetic Interference) will affect Twinax

    - SFP+ Active Optical Cable(AOC)
      - almost similar to Twinax cables
      - in lengths of 1,2,3,5,7 and 10 meters
      - EMI won't affect AOC



  * SFP28 Modules 
     - Twinax(DAC)
     - Active Optical Cable(AOC)
     - 25G Short Reach (SR) Module
     - 10/25G Long Reach (LR) Module
     - 10/25G Cisco Short Reach (CSR) Module




** 40GG and 100G Transceiver Options
  * 40G QSFP Modules
    - QSFP-40G-SR4-S
    - QSFP-40G-SR4
    - QSFP-40G-SR-BD

** QSFP-DD(Double Density) Transceivers
  - QSFP-DD is backward compatible with QSFP+/QSFP28 transceivers
  - QSFP-DD400
    - supports 400G speed
    - provides 8 lanes of 50G for 400G total
  - QSFP-DD800
    - supports 800G speed
    - provides 8 lanes of 100G for 800G total

** Verify Compatibility with Cisco TMG
  - check "Cisco Optics-to-Device Compatibility Matrix" or "Transceiver Module Group Compatibility Matrix" to see if the device actually supports module
  - https://tmgmatrix.cisco.com/




~~~~~~~~~~~~~~~~~ Describe the ACI Logical Model ~~~~~~~~~~~~~~~~
** Mapping the Logical Models
  - The logical Model represents how traffic flows through the ACI domain 
  - EPG(EndPoint Group)
    - it's a collection of EndPoints
    - Endpoints in an EPG don't have to be in a same subnet
    - All of the devices in an EPG can communicate with eath other freely
    - by default we don't allow any traffic betweeen EndPoints in different EPGs
    - A Contract defines how EndPoints in two different EPGs can communicate with eath other
    - Bridge Domain(BD)
      - can contain a set of components like subnets, GW, EPGs
    - Context/VRF
      - each VRF includes some Bridge Domains
    - Tenant
      - each Tenant can include some VRFs
      - ACI is a MultiTenant infrastructure


** Endpoint Groups and Contracts
  - Cisco EndPoint Types
    1) Physical Device Types
    2) Virtual EndPoint(VM, Virtual Appliance, VCenter, ...)
    3) External Devices(Users)

  - Cisco EndPoint Group Types
    1) Application
    2) External(for External Endpoints/Users)
    3) Micro Segmentation  

  - Ways to assign EndPoints to EndPoint Groups
    - Physical Port, VLAN
    - Source IP
    - Source MAC
    - VMM Integration

  * Contracts
    - Contracts allow Inter-EPG traffic
    - Consists of multiple subjects and filters
      - each subject includes Filters
        - Ports, Protocols
    - We can apply contracts between two EPGs in different Bridge Domains(BD)
    - We should set the direction of contract
      - in each contract, there is a Provider and a Consumer of the contract
      - The provider tells the Consumer here is how you're allowed to communicate with me
        - the direction of the Traffic in a Contract is from Consumer towards the provider
      - The contract is Unidirectional 


** Bridge Domains and Application Network Profiles
  * Bridge Domain
    - Bridge Domains represent a Broadcast Domain
    - Subnets and Anycast Gateways are configured within BDs 
    - We need Unique Mac Addresses
    - We have Broadcast Traffic
    

  * APN(Application Network Profile) OR AP(Application Profile)
    - contains:
      - EPGs
      - Contracts
      - Service Graphs
    - ANP represents an Application, and it maps EPGs to BDs


** Contexts and Tenants
  - ACI Context is equivalent to VRF
  - Bridge Domains belong to one VRF
  - Tenant is in Management Layer
  - Default Tenants inside ACI Infrastructure
    1) Common: Any construct located here belongs to All Tenants
    2) Infrastructure: responsible for Leaf-Spine Connectivity
    3) Management: Handle all of InBand and OutOfBand(OOB) Management Interfaces





~~~~~~~~~~~~~~~~~ Describe the ACI Physical Model ~~~~~~~~~~~~~~~~
** Physical Model Overview and Benefits
  - The ACI Physical Model defines how physical policy is applied
  - One great advantage of ACI is scaling to very large data centers
  - ACI policy takes longer to create, but it is only done once


** VLAN Pools and Domains
    - VLAN Pools are how we we assign a group of VLANs to interfaces
    - VLAN Pools can be Static or Dynamic and they are mapped to a domain
    - Domains describe how an endpoint connects
    - 5 Types of Domains:
      1) Physical
        - it's when we have a virtual host or bare-metal server type of application connected on a physical port
      2) Virtual
        - it will come from VMM(Virtual Machine Manager) Integration
        - VMs will be part of Virtual Domain
      3) External Bridged(L2)
      4) External Routed(L3)
      5) Fiber Channel
        - Storage Area Networking(SAN) technology(i.e. it's gonna connect us to Storage Arrays)
        - Cisco 9Ks support FCoE and FC


** Interface Policies, Interface Policy Groups, and AAEPs
  - Interface Polices
    - configurations like cdp, lldp, speed, etc. on interface level

  - Interface Policy Groups
    - a set of Interface Policies inside a group
    - Types of Interface Policy Groups:
      1) Access
        - Access port on the Edge 
        - Single Interface Configuration that's going at the edge 
      2) Port-Channel
      3) Virtual Port-Channel(VPC)

  - AAEP(Attachable Access Entity Profile)
    - It will allow us to connect different objects(that we've created) such as domains to another object such as Interface Policy Groups(EPGs)




** Interface and Switch Profiles
  - Interface Profile
    - it's essentially a template
    - Interface Profiles map an Interface Policy Group to a range of interfaces

  - Switch Profile
    - Switch profiles map a set of Interface Profiles to a range of switches
    - We can use an Interface Override to inject custom configuration


~~~~~~~~~~~~~~~~~ Apply ACI Concepts ~~~~~~~~~~~~~~~~
** Fabric Setup
  - When setting up a fabric, we start by registring the switches
  - We then configure BGP RRs, OOB Mgmt, NTP, DNS, etc
  - We can validate our topology on the Fabric -> Topology Tab 

** ACI Fabric Routing
  - IS-IS is used for routing across leaf-spine fabric
  - VXLAN is used for L2 propagation, with COOP at the control plane
  - BGP propagates learned routes to other leaf switches, spines are RRs(Route Reflectors)

** Tenant and Access Policies
  - Tenant Policies are the ACI Logical Model policies
  - Fabric Policies are used on the fabric connections
  - Access Polices are the ACI Physical Model policies


** VMM(Virtual Machine Manager) Integration
  - We integrate to a VMM from the APIC, leverages APIs for control 
  - There are 3 ways for VMM Integration
    1) Native
      - VLAN
    2) Application Virtual Edge(AVE)
      - Cisco VS
      - Virtual Leaf
    3) Application Virtual Switch(AVS)
      - it's a Legacy option
      - Nexus 1000V
  - Traffic flow looks different for the different switching options


** Microsegmentation in ACI
  - Microsegmentation in ACI refers to granulated policy deployment
  - Microsegmentation EPG
    - specify VMs, and ACI pulls those VMs into the microsegmatation EPG
    - VMM domains
    - contracts can be built from the microsegmentation EPGs
  - Ways to identify the microsegmented EPGs
    - Network: IP/MAC
    - VM based attributes: VMM domain, OS, VM ID/Name, Custom Tags, ... 
