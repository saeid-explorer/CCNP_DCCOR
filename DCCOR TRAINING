====================================================================================== CCNP DCCOR - CBT Nuggets - Jeff Kish ================================================================================


~~~~~~~~~~~~~~~~~ Explain BGP Concepts~~~~~~~~~~~~~~~~

** BGP Concepts
  - BGP is an EGP(Exterior Gateway Protocol)
  - EGP is designed to be used between larger networks versus IGP(Interior Gateway Protocl) which is designed to be used within a network(e.g. OSPF)
  - IGPs are mode for speed(They converge rapidly) compared to EGPs which focus on Scale
  - IGPs are Distance Vector or Link State protocols but BGP is Path Vector protocol 
  - BGP has 10+ path attributes which are used for routing

** BGP Peerings
  - AS(Autonomous System) is 16bit number which accounts for 65K AS
  - There are two types of Peerings:
    - eBGP: peering between routers in different ASs
      - AD(Administrative Distance) is 20
    - iBGP: peering which happens within an AS
      - AD(Administrative Distance) is 200
  - BGP peerings are formed across a TCP connection on port 179
    - This means two routers without a direct connection can still form relationship
  - Peers are identified by a Router ID(32bit)
    - Router ID can be assigned manually or Automatically(using the highest Loopback IP or highest other Interface IP address)


** BGP Path Selection
  - Note: Path must be Reachable
  - Here are path selection criteria/attributes sorted by their priority:
      1) Weight
          - the higher the better
          - Cisco-only parameter
          - specifically local to the router
      2) Local Preference
          - the higher the better
          - it's globally shared within an AS
          - in other words, it's a weight that is shared
      3) Locally Originated
          -  prefer the local path to the neighbors' advertised path
      4) AS-PATH
          - the less/shorter the better
          - for example AS-path 400 wins over AS-Path 200-300
      5) Origin
          - the lower the better
          - Origin explains how a Route got into BGP in the first place
            - for example we might use the "Network" command which injects the route natively into BGP (marked by "i" in Routing Table)
            - we might also see "?"(i.e. Incomplete) in Routing table which means the Route is redistributed into BGP 
            - so "i" Origin is prefered over "?" Origin
      6) MED(Multi-Exit Discriminator)
          - the lower the better
          - it's actual cost or metric that we get in BGP
          - this is used to tell others how to come back into my network
          - for example when I have a Router on the Edge of my AS and I have 2 different Paths 2 different Routers in a different AS,
            I can use MED to influence the inboud direction of the packet
      7) eBGP > iBGP Peer
          - Routes received from eBGP peers are preferred over iBGP Routes
      8) Next-Hop - IGP Metric 
      9) Last chosen
      10) Peer Router-ID (Optional)
          - the lower the better
      11) Cluster Length
          - the lower the better
          - Relates to RR(Route Reflector)
      12) Peer IP Address
          - the lower the better


** MP-BGP (MultiProtocol BGP)
  - BGP can route IPv4 and IPv6
  - BGP is an Information Carrying protocol which can protocol lots of information types(e.g. IPv4/IPv6 Multicast, Mac Address)
  - we set the protocol that we want to carry as a subcommand under "router bgp <As#>" command
      - e.g.:
        - router bgp 65500
          - address-family ipv6


~~~~~~~~~~~~~~~~~ Explain Multicast Routing ~~~~~~~~~~~~~~~~

** Multicast Traffic Concepts
  - we have three components
      - Multicast source
      - Multicast receiver
      - Group Address: A class D IP Address that we are sending this multicast traffic stream to
        - Class D Address is in the range of: 224.0.0.0 to 239.255.255.255
        - 239.0.0.0 is the range of private multicast group IDs
  - different types of multicast traffic groups:
        1) Any-Source Multicast(ASM)
            - It's the situation in which we don't care which Source gives us the traffic stream
        2) Source-Specific Multicast(SSM)
            - Allows us to reach out to very specific source on the network and request the source that way
            - It greatly simplifies our multicast deployment because the host knows where exactly the source is
        3) BiDirectional
            - Designed for the situations when we have a lot of sources and a lot of recipients

  - L2 and L3 Multicast
      - L3 Multicast is going to be managed by a protocol called PIM(Protocol Independent Multicast)
          - The goal of PIM is to create Loop-free topology through the entire network that we call it Tree
          - This tree is going to connect the sources to the destinations in a loop-free way inside the network
      - L2 Multicast uses IGMP(Internet Group Management Protocol)
          - IGMP is a protocol that allows clients to request connections to specific multicast streams
          - We can also use a concept called IGMP Snooping in order to prevent Broadcast behaviour



** Protocol Independent Multicast(PIM)
  - PIM's responsibility is to connect the Sources with whatever clients want to receive that particular Multicast stream
  - It's protocol independent because we're relying on a Unicast Routing Protocol to already have been established for us to have
    converged routing(e.g. OSPF, EIGRP) in the Unicast Environment
  - PIM's domain of operation is from Ingress router to Egress router
  - Its responsible for L3 domain of Multicast
  - PIM routers should start by forming PIM neighborship with one another
  - Routers are going to discover each other with a multicast IPv4 address of 224.0.0.13 on IPv6 of FF02::D
  - Multicast Routes are created when a new Source shows up and sends Multicast or when a Receiver shows up and want to join a particular Multicast group
  - command to show Multicast Routing Table:
      - show ip mroute
  - 2 different types of Multicast routes on Multicast Routing Table
      - "S" stands for Source, "G" stands for Group, "*" means we don't know what the source is
      1) (S,G)
          - They are going to be established before (*,G) group in many cases as going to tell us what to do when we finally see where the source lives
          - e.g.: (10.1.1.1, 239.1.1.1)
      2) (*,G)
          - These Routes are kind of like Template that tell us what to do when the (S,G) Routes finally arrived
          - If the client first sends a Join request(to join a Multicast group) to the router
          - Once the router received the request, it's gonna build (*,G) entry into the Multicast Routing Table(e.g. (*,239.1.1.1) )
          - Note: with any Multicast Route, we need to identify Incoming Interface(II) and the Outgoing Interface List(OIL)
          - for (*,G) Routes, II is always "NULL" because in this case we don't have a Source

  - 2 Operation Modes of PIM
      1) DENSE Mode
          - has a Flood and Prune methodology(Flood the traffic and Prune it back)
          - We don't use DENSE Mode onto Cisco Nexus operations
      2) SPARSE Mode
          - On the Cisco Data Center we have only Sparse Mode available to us
          - It's going to use a lot of Intelligence to figure out where those Sources are
    - These Modes are going to be how we discover where the Sources are 


** PIM SPARSE Mode Operation
  - Rendevous Point(RP): This is going to be the contact for both the Sender and the Receivers
  - We are going to call a Router as RP and it will be known by every single router in the network
  - So every Router(Sender&Receiver) inform RP about their Needs/Requests
  - If the receiver shows up first:
      - Receiver will send a join request to its upstream Router and that router sends the join request to RP to join a specific Multicast group(e.g. 239.1.1.1)
      - RP gonna install (*,G) entry in its Multicast Routing table
  - If the Source show up:
      - It will ask RP for sending a Multicast traffic for specific Multicast group
      - The RP will install (S,G) Route in its Multicast Routing table
      - Then the RP will bind (*,G) Routes with (S,G) Routes and send the Multicast Stream to the given Clients in the specific Group


** PIM Roles
    1) Rendevouz Point(RP)
        - Contact for connecting the Sources and Receivers
        - We can Statically set any Router as RP or we can Automatically set a Router as RP in one of two following ways:
            1) Auto-RP
                - deployed by Cisco
                - based on SPARSE-DENSE Approach of PIM which is now obsolete in the Data Centers 
            2) BSR(BootStrap Router)
                - it's included with PIMv2
                - It's industry standard that's built-into PIM now
                - It uses PIM messaging sent to 224.0.0.13
                - BSR is a Mapping Agent(MA). It accepts all the messages from the Routers that want to be RP and sends those messages to 
                  the rest of the network via native PIM messaging
            3) Anycast RP
                - It leverages the concept of Anycast
                - It's going to use exact same IP Address on two different Routers
                - Here we have redundancy
                - The two routers(with same IP) use a protocol called MSDP(Multicast Source Discovery Protocol) to communicate with each other
                  to allow each other to know about any Sources that they become aware of. This way we have one Synchronized database of Sources
                  between two RPs
    2) PIM DR(Designated Router)
      - This is when we have two Routers in one network segment
      - PIM DR is the Router that is responsible for sending client join Requests to the upstream Router
      - it's elected based on the highest IP Address on that subnet
    3) PIM DF(Designated Forwarded)
      - if we receive a Multicast Stream on two different Routers that are on the same Segment which should forward the Stream to Downstream device(Router/Client)
      - PIM DF is responsible for forwaring this Multicast Stream
      - In this case the two competing routers are going to enter into a process called PIM ASSERT
      - The Router which wins the PIM ASSERT process gonna forward the Multicast Stream




** PIM Configuration
    - We have two types of configuration to do:
      1) Global Configuration
          # Feature pim
          # ip pim rp-address
      2) Interface-level Configuration
          # ip pim sparse-mode
          - To enable Authentication
            # ip pim hello-authentication ah-md5 <password>
          - To make a specific Router to be DR
            # ip pim dr-priority
          - To configure Auto-RP and select a Router as RP candidate
            # ip pim auto-rp rp-candidate
          - To configure BSR and select a Router as RP candidate
            # ip pim rp-candidate
          - To configure a Router to be as MA(Mapping Agent) in Auto-RP
            # ip pim auto-rp mapping-agent
          - To configure a Router to be as BSR in BSR mode
            # ip pim bsr-candidate
          - To enable the forwarding of RP related traffic on Auto-RP mode
            # ip pim auto-rp listen forward
          - To enable the forwarding of RP related traffic on BSR mode
            # ip pim bsr listen forward

    - show Multicast Routing Table
          - show ip mroute

    - show pim info on interface level
          - show ip pim neighbor
          - show ip pim interface

    - flush all Multicast Routes out and restart all pim adjacencies
          


** IGMP and MLD
    - The role of IGMP is to allow the hosts to request for participating a particular Multicast Group
    - The role of MLD is the same as IGMP but for IPv6
    - host/client gonna send a request for joining the Multicast Group. This request is called "IGMP Report" and its destined to the Multicast Group Address(e.g. 239.1.1.1)
    - IGMP Querier
      - if we have more than one Router, the router with lower IP Address will be IGMP Querier 
    - IGMP has two versions that are supported on Cisco Nexus Switch
      - IGMP v2
      - IGMP v3
        - supports SSM(Source-Specific Multicast). i.e. when the host sends IGMP Report, it says from which Source it wants to receive Multicast Stream
    
    - MLD has two versions that are supported on Cisco Nexus Switch
      - MLD v1
      - MLD v2
        - supports SSM

    - IGMP Snooping
      - Multicast in layer 2 acts like broadcast, so we use IGMP Snooping to make it act like Multicast
      - using IGMP Snooping, the switch sends the Multicast Streams only to the hosts that have requested


** IGMP and MLD Configuration
    # feature pim/pim6
    - under the interface
        # ip pim sparse-mode
        # ip igmp version 3
        # ip igmp join-group
          or
        # ipv6 mld join-group
    - Validating commands
        # show ip igmp interface
        # show ipv6 mld interface
        # show ip igmp snooping groups
        # show ipv6 mld snooping statistics int vlan <vlan#>



~~~~~~~~~~~~~~~~~ Explain Virtual Port Channel ~~~~~~~~~~~~~~~~
  
  - Nexus Port-channels are different from IOS devices
  - Configuration options are static(ON) and LACP(active/passive)
  - Load-balancing across port-channel links is hash-based

  * Note: In VPC we can only support exactly 2 upstream Nexus Switch
  * Note: VPC is only L2

  * VPC Domain
    - it is assigned a unique number inside your datacenter environment

  * Peers
    - Two Nexus devices in the VPC domain are called PEERS
    - one of the devices is Primary Peer and the other is Secondary Peer

  * KeepAlive Link
    - it's going to be a dedicated connection between peers that is part of its own VRF
    - This should be a L3 connection
    - Peers will send KeepAlive messages to one another to make sure whether the other side is up

  * Peer-Link
    - it's a connection between peers which is usually several physical connections with Standard Port-Channel
    - peers will use this link for control plane information
    - Control Plane uses a Protocol called CFS(Cisco Fabric Services)
    - any VLAN that we extend along a VPC must be included on the Peer-Link
    - The general practice is not to restrict VLANs on the Peer-Link

  * VPC Member Ports
    - The ports that are part of VPC

  * VPC Orphan Ports
    - Any port in a VPC environment that aren't part of VPC

  * Note: VPCs give us the resiliency and storage connections needs in the DC

  * Consistency Checks of VPC
      - If there were inconsistencies in Type 1 checks, the VPC wouldn't come up
      - If there were inconsistencies in Type 2 checks, Port-Channel will come up but you'll receive some errors and problems
      1) Type 1
          - STP Type
          - Access/Trunk
          - Native VLAN
          - Speed

      2) Type 2
          - SVI UP/DOWN
          - QoS Configuration
          - VLAN Allowed List  


    * Note: VPC traffic is hashed, but only local VPC port members are considered

    * Note: Loop Prevention blocks Peer Link packets from forwarding out a VPC

    * Note: FHRPs run in an Active/Active state to resolve black hole scenarios

** VPC and STP
    - In normal operation, the VPC Primary handles BPDUs on VPCs
    - Peer-Switch is a concept which means two VPC switch peer in order to be considered one logical switch
    - Using Peer-Swtich, the peers share a bridge ID for VPC links and that enables both of them to send and receive BPDUs
    - Peer-Switch is designed for when one of the peers is STP Root

** VPC Topologies
    - Bow-Tie
      - it's a topology in which two downstream switch connect to two upstream routers using VPC
      - we have two paris of switches
      - The link between the pairs is L2

    * Note: L3 adjacencies to the VPC peers should not be formed over a VPC
    * Note: Use dedicated L3 links when routing adjacencies to the peers are required

** Configuring a VPC Domain
    - do the following configuration on both Nexus Switches:
      - feature vpc
      - vpc domain <a unique number>
      - peer-keepalive destination <destination IP of keepalive link> source <Source IP of keepalive link>
    - to validate the config use the following command and check "vPC keep-alive status"
      - show vpc
     - Go inside the interface of peer-link on both Switches
      - int <interface_number>
          - switch mode trunk
          - channel-group <number> mode <on/active>
      - Go to the port-channel that you've created
        - int port-channel <number>
          - vpc peer-link
      - To validate the peer-link, see if "peer status" is "peer adjacency formed ok" using following command
        - show vpc
      - Go to the interfaces that you want to configure as the members of vPC ports to downstream swtich
        - int <interface_name>
          - switchport mode trunk
          - channel-group <number> mode active
              * Note: you should enable LACP feature first(i.e "feature lacp")
      - Then go to the newly created port-channel
        - int port-channel <number>
          - vpc <number>
              *Note: for the human-readability it's better to match vpc and port-channel number of vPC member ports
      - Then go to downstream switch(e.g. Catalyst) and enter into ports that will join the vPC
        - int range <interface range>
          - switchport trunk encapsulation dot1q
          - switchport mode trunk
          - channel-group <number> mode passive

** VPC Roles and Failover
    - one of the Nexus VPC switches will be Primary and the other will be Secondary
    - Primary and Secondary roles will be determined via an Election
    - To see the role of each Nexus switch
      - show vpc role
    - Election will be done one time when vPC domain comes up
    - Election will be based on Priority and Mac Address
    - There is no Preemption. It means if the primary failed, it won't get Primary again after comming up

    * Failure Scenarios
      1) Peer-Link is down, but KeepAlive link is up
        - In this case, the secondary switch will shut its vpc link down

      2) Peer-Link and KeepAlive links are both DOWN
        - In this case, secondary switch will keep its VPC up and it will assume the role of operational primary, it
          means it's still configured as secondary, but it's acting as if it's primary

      3) Peer-link is UP, but KeepAlive links are Down
        - we have to solve KeepAlive link problem right away, but it's not going to affect Data Plane Operations

** Configuring VPC Roles
    - to make a nexus switch to be VPC Primary, use the priority command and set it to a lower priority, and set the secondary swith to have higher priority
    - e.g.:
      (in primary switch)
      - vpc domain <domain_number>
        - role priority 10
      (in secondary switch)
      - vpc domain <domain_number>
        - role priority 20
    * Note: we should either shut down vpc links and "no shut" them again or do the following to take effect:
        - vpc role preempt


** VPC Configuration Options

  * ARP Syncronization
    - This gonna make sure that we're dynamically and actively/proactively exchanging ARP entries of the Nexus Switches with one another
    - That way in a failure event, we don't have to re-ARP for bunch of entries that we weren't even using before the failure
    - We should enable ARP Sync in VPC domain
    - The command to enable it
      - vpc domain <number>
        - ip arp synchronize

  * Auto Recovery
    - It's one of those configuration parameters that is actually enabled by default
    - In Auto Recovery we gonna wait for 240 seconds(i.e. 4min) in hope that the other failed switch(primary) comming online
        - if the other failed switch didn't came up in 240 seconds, the switch will take the Primary role

  * Peer Gateway
    - It extends FHRP black hole prevention to SVIs
    - The command to enable it
      -  vpc domain <number>
        - peer-gateway



  ~~~~~~~~~~~~~~~~~ Describe Data Center FHRPs ~~~~~~~~~~~~~~~~

** First Hop Redundancy Protocols(FHRPs)
    - these protocols allow for network-based gateway redundancy
    - using these protocols we can have two gateways for our clients and assign a single VIP for the gateways
    - Protocols:
      - HSRP(Hot Standby Router Protocol) 
        - Cisco proprietory
        - two versions: v1(original version), v2(optimized version)
        - default version of HSRP on Cisco NX-OS is v1

      - VRRP(Virtual Router Redundancy Protocol)
        - It's industry/open standard
        - versions: v1(obsolete), v2 and v3
        - default version of VRRP on NX-OS is v2

      - GLBP(Gateway Load Balancing Protocol)
        - Cisco proprietory
        - it's focus is on load balancing
        - it's supported on Nexus 7K but not on Nexus 9K



** Hot Standby Router Protocol(HSRP)

  - In HSRP we will have groups containing routers
  - groups will be configured on Interface
  - Each group will be assigned an ID
  - Each router will be placed in one of these states:
    - Active
    - Standby
    - Listen

  * Priority
    - it's a configurable value in the range of 0-255
    - Default is 100
    - If the priority of the routers is the same, the router with "Highest IP Address" wins(becomes Active Router)

  * Timers
    - The routers send messages to one another
    - sending these messages is according to "Hello Timer"
    - Hello Timer is by default 3 seconds
    - Hold Timer is by default 10 seconds
    - Hold Timers says us how long we can go without receiving a hello message before we assume the other side is down
    * Note: we can use BFD(Bidirectional Failure Detection) algorithm to detect the failures even sooner (down to 50ms)

  * Preemption
    - it needs to be enabled if I want a router to dynamically take over(become Active router) for example as a result of changing priority value
    - It's disabled in HSRP by default

  * Security
    - Authentication
      - we can for authentication between routers inside a HSRP group
      - The password can be in two froms
        - Plain text
        - MD5

  * HSRP v2
    - It has the possibility of more group IDs(4096)
    - We are able to tune our timers down to Millisecond range(as low as 250ms)
    - Multicast address is going to change
      - In HSRP v1, Multicast Address is 224.0.0.2
      - In HSRP v2, Multicast Address is 224.0.0.102 

  * Note: HSRP is Active/Active in VPC topologies


** Virtual Router Redundancy Protocol(VRRP)
    - Unlike HSRP, the roles are:
      - Master
      - Backup

    - Priority
      - in the range of 1-254

    - VIP
      - we can select the IP address of Master as VIP

    - Preemption
      - is enabled by default

    - Group ID
      - In both VRRP v2 and v3 there are 256 Group IDs


** FHRP Object Tracking
    - Object Tracking enables FHRPs to react to indirect failures
    - Devices can track a number of different parameters
      - Route
      - Line Protocol
      - SLA
      - List

    - While tracking the parameters
      - when a problem is detected, the device will decrement its priority
      - in this case we expect the backup/standby router to take over the Master/Active Role
      - preemption should be enabled


** Configuring HSRP
    - first enable the feature on Nexus Switch
      - feature hsrp
    - then configure hsrp on the interface as the following example
      - int vlan 1
        - hsrp version 2
        - hsrp 1
          - ip 192.168.1.254 (VIP)
    - to change priority and turning Preemption on
      - int vlan 1
        - hsrp 1
          - priority 110
          - preempt

    - to change Hello time and Hold time(e.g. 1s for Hello time and 3s for Hold time)
      - int vlan 1
        - hsrp 1
          - timers 1 3

    - to change Hello time and Hold time(e.g. 250msec for Hello time and 750ms for Hold time)
          
    - hsrp verification
      - show hsrp brief


** Configuring VRRP
    - first enable the feature on Nexus Switch
      - feature vrrp
        or
      - feature vrrp3

    - then configure vrrp on the interface as the following example
      - int vlan 3
        - vrrp 3
          - address 192.168.3.254 (VIP)
          - no shut
            - we have to do no shut, unless vless won't get enabled

     - vrrp verification
      - show vrrp

    - to change priority and turning Preemption on
      - int vlan 3
        - vrrp 3
          - priority 110
          - preempt

    * Note: When configuring VRRP to use a rouer's physical IP address (as VIP), the priority value of 255 is automatically assigned



~~~~~~~~~~~~~~~~~ Explain Overlay Transport Virtualization(OTV) ~~~~~~~~~~~~~~~~

  ** OTV Architecture
    - OTV is designed to extend L2 to many different locations across L3 boundaries (Data Plane Operation)
    - It relies on IS-IS routing protocol to do what is called as MAC Routing (Control Plane Operation)

    * Join Interface
      - It's physical or logical interface that connects us to our L3 Network(It's where IP Address lives)

    * Internal Interfaces
      - They are usually L2 extensions(usually Trunk connections) to the rest of the Data Center(Where hosts and all the entities on Internal network connect to)

    * Overlay Interface
      - It's a logical Tunneling interface that's going to exist on the Edge device
      - This is how we are going to form our tunnels across the L2 boundary
      - we are going to form a tunnel between the overlay interfaces of two edge devices on different Data Centers
      - They don't have IP addresses set on them, instead they borrow the IP of Join Interfaces for source and destination of OTV Tunnel
      - The tunnel is going to carry L2 Frame inside L3 Packet

    - The Cisco Platforms which support OTV
      - Nexus 7K
        - The Line Cards must be either M series or F3 or above 
        - We need to deploy a seperate VDC in order to accomplish this
        - OTV itself requires a seperate License

      - ASR 1000
      - CSR1000V
      - ISR 4K 



** OTV Control Plane
  - IS-IS routing protocol is used in OTV Control Plane
  - IS-IS is often used on Service Provider space
  - IS-IS can carry any type of information
  - IS-IS is based on TLV(Type,Length,Value) fields(i.e. we don't have to carry Routes, we can carry MAC Addresses)
  - Cisco is going to do the IS-IS config for us on the backend(automated by OTV)
    - we can also manually set Timers or Authentication
  - OTV shares updates as MAC addresses are learned and pulled
  - each devices will advertise the MAC addresses in its CAM table to the other devices
  - Adjacencies are formed via the control-group Multicast address ideally
  - For unicast transports, we use adjacency servers to form adjacencies



** OTV Data Plane
  - Unicast traffic is handled via unicast in the transport network
  - BUM(Broadcast, Unknown Unicast, Multicast) traffic is also sent via unicast to all destinations if transport is unicast
  - With a multicast transport, BU traffic is sent via control-group ASM(Any Source Multicast) address
  - Multicast(L2) traffic uses SSM(Source-Specific Multicast), which requires IGMPv3 on the Join Interface



** Multihoming and Authoritative Edge Devices(AEDs)
  - BPDUs are not propagated across the OTV overlay
  - One of the devices in each DataCenter/Subnet will be assigned as AED for a particular VLAN 
  - AED is going to affect both Control Plane and Data Plane
  - In Control Plane perspective, A device will only advertise the MAC addresses for a VLAN if it's the AED for that VLAN
    - Also The device will only send the traffic to it's VLAN, if only it's the AED for that VLAN
  - VLANs are automatically mapped based on evens and odds
  - Backdoor links will require manual configuration
  - Site-VLAN
    - it has to be a dedicated VLAN that connects two AEDs as directly as possible
  - The AEDs can also become backup AED for other Vlans 


** OTV Design Considerations
    - ARP Optimization
      - reduces flooding behavior of ARP between sites
    - OTV adds 42Bytes to MTU(MTU becomes 1542Bytes)
    - OTV sets DNF(Do Not Fragment) bit in a packet
    - FHRP Isolation
      - We have to create VACL(Vlan Access Control List) and block FHRP Mac Addresses between FHRPs on two different Data Centers
      - FHRP Nodes both become Active and we gonna have AnyCast Gateway
  
  ~~~~~~~~~~~~~~~~~ Explain VXLAN and EVPN ~~~~~~~~~~~~~~~~

** VXLAN and VNI
  - There are some problems in traditional Data Centers
    - Vlans are limited (4K)
    - We have to use STP, FHRP, VPC which have their own problems
    - We have to use L3 PODs (L3 at edge) which are problematic

 - VXLAN helps us to better scale our network by two primary ways
    1) It will increase the number of Broadcast Domains that are available to us
    2) It's gonna extend L2 over L3

 - When we enable VXLAN on a switch, it's become known as VTEP(VXLAN Tunneling EndPoint)
 - One VLAN is going to be mapped to a single VNI(Virtual Network Identifier)
 - Then each VNI is going to be extended through a L3 boundary and connect to another VTEP which receives the 
   VNIs which has encapsulated the traffic that VXLAN enabled us to send.
 - Then the destination VTEP is going to translate VNI back into the individual VLANs that were originally
   connecting with.

 - VXLAN header is a 8 Byte header
    - 1B -> Flags
    - 3B -> VNI (almost 16Million VLANs)
    - 4B -> Reserved


  - Scoping the VNIs
    - There are two primary ways of scoping VNIs
      1) Network Scope
      2) Local Scope(locally significant VNI)
      
      - Network-wide scope means that if we were to apply a VNI to a particular VLAN, the question we're
        going to ask is whether this VNI is significant only to the local switch or to the entire Network.
      - Scoping Network-wide means that the specific VNI will also get mapped to the same Vlan on the
        destination VTEP
      - Benefit of Network-wide Scoping is that it's SIMPLE(i.e. VNI gonna point to the same VLAN in the
        entire Network)
      - Downside of Network-wide Scoping is Management. Because we have to be a worry about that the specific
        VNI is reserved everywhere and we can't repurpose that VNI by accident anywhere in the Network.


** VXLAN and Leaf-Spine
  - Leaf-Spine is only doable because we have VXLAN

  - Leaf-Spine Architecture
    - We have two layers: 1st layer is Spine and 2nd layer is Leaf
    - we connect every leaf switch to every upstream spine switch
      - we don't connect leaf-to-leaf or spine-to-spine(unline Aggregation-Access Architecture)
    - All of the Intelligence lives at the LEAF layer, not the Spine layer
    - The Spine switch has only one task: to bring traffic in and send back out which enables us to switch traffic
      between Leaf swtiches as fast as possible
    - Leaf-Spine is designed for scaling to very large L2 Fabric in the DataCenters today

  - Leaf-Spine Scaling
    1) we have Independent Scaling of different resources in the data center
        - we have a Backbone layer(Spine Layer) and if we want to increase our Bandwidth or PPS(Packets Per Second),
          we add new Spine Layer Switch
        - If we need Ports(additional connectivity at the Edge), we simply add Leaf switches
    2) Declaring Two-Hop behaviour for all Traffic in Data Center
        - When a packet arrives on a Leaf Switch, it's guaranteed that regardless of where it's heading in the Data Center,
          it's going to get there in precisely 2 Hops(first to Spine and then to the destination Leaf Swtich)

    3) Services
        - We can make our Network Services available very seamlessly throughout the Infrastructure
        - We bring our services(Network Load Balancers, Cloud Gateways, NGFW, AnyCast Gateways) into a pair of leaf switches


  - Application of VXLAN in Leaf-Spine Architecture
        - The ideal situation in Leaf-Spine Architecture is that the connections between Leaf to Spine to be L3(to be able
          to scale this architecture out) to be able to scale out. Otherwise in L2 connections STP gonna cause lot of problems
        - In this case, we can have two hosts on two different Leaf switches which are in the same VLAN. Then we can have these
          subnets stretched out(L2) because they are in the same VLAN and yet our Fabric is L3



** VXLAN Data Plane
    - VXLAN gonnna extend L2 over L3 using Encapsulation
    - The L2 Frame gonna come to source VTEP. Then it will be encapsulated into VXLAN Header
    - Then it will be encapsulated inside UDP Header which has Destination Port of 4789
    - Then It will be encapsulated into IP Header which has Source and Destination IP of Source and Destination VTEPs
    - Then it will send the Encapsulated packet over L3 to the destination VTEP
    - Destination VTEP gonna decapsulate it and get the packet
    - Fragmentaion is not supported by VXLAN
    - Remember increasing MTU

   - MTU increase required on the VXLAN underlay network
      - IP header       -> 20B
      - UDP header      -> 8B
      - VXLAN header    -> 8B
      - Ethernet header -> 14
      - So totally we should add 50 Bytes to the size of Data packet to get our MTU size

** VXLAN Control Plane - Flood and Learn
    - VXLAN lacks a native, dedicated Control Plane, instead it relies on Flood and Learn
    - Traffic is flooded via Multicast across the backbone to all VTEPs
    - Once a destination is learned, the ingress VTEP can forward as unicast
    * BUM(Broadcast, Unknown Unicast, Multicast) Traffic will be sent using  Multicast tree
    - IR(Ingress Replication) is where we have Unicast Backplane
      - it's an alternative method of control plane(for Multicast) which is rarely used
      - we have to manually map VNI to VTEP which takes lots of work
      - we gonna replicate lots of traffic


** VXLAN Control Plane - Ethernet VPN (EVPN)
    - BGP is more of a Information Carrying Protocol(rather than simply a Routing Protocol)
    - MP-BGP(Multi-Protocol BGP) can carry IPv6, MPLS or even MAC and VNI
    - EVPN deploys MP-BGP to exchange IP/MAC/VNI info among VTEPs
    - BGP should also include a RR(Route Reflector) as part of EVPN designs
    - EVPN brings enhancements like Peering(Authentication) and ARP Suppression(Prevent flooding ARP Requests)
    - BUM traffic continues to use Multicast for multi-VTEP delivery


~~~~~~~~~~~~~~~~~ Configure Overlay Transport Protocol(OTV) ~~~~~~~~~~~~~~~~

** Setting up the OTV Environment
- Configurations on the device on the transit environment of OTV(e.g. CSR1000v)
  - configure multicast routing on the device on the transit environment of OTV(e.g. CSR1000v)
      # ip muticast-routing distributed
  
    - Assign an IP address to it's multicast port
      # ip pim rp-address 1.1.1.1
  
    - enable pim sparse-mode on loopback interface
      # ip pim sparse-mode
      # ip address 1.1.1.1 255.255.255.255

    - enable igmp version 3 on the interfaces towards two other downstream devices(e.g. Nexus 7K)
      # ip igmp version 3


- Configuring Internal and Join Interfaces of OTV on Cisco Nexus 7K
  - Go to the L2 interface facing the upstream CSR1000v and configure it as JOIN interface
    # int e2/1
      # ip add 10.0.0.2/30
      # mtu 9216
      # ip igmp version 3
      # no shut 

  - Go to the interface facing the Local Network and configure it as Internal interface
    # int e2/2
      # switchport mode trunk
      # switchport trunk allowed vlan 10-20
      # no shut
      # exit

  - Create vlans
    # vlan 10-20
    # vlan 5
    # exit

      
** Configuring OTV
